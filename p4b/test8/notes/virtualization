PROCESS:

	mechanisms and policies

	C's stack: local variables, function parameters, return addresses

	C's heap: dynamically-allocated data

	process states: running, ready, blocked

	fork(), wait(), and exec()



MECHANISM: LIMITED DIRECT EXECUTION:

	user mode and kernel mode

	use trap instruction

	use timer interrupt to help os maintain control of the machine

	during a timer interrupt or system call, use context switch to change process


SCHEDULING: INTRODUCTION:

	two scheduling metrics: turearound time, response time

	five assumption:
		
		1. each job runs for the same amount of time
		
		2. all jobs arrive at the same time
		
		3. once started, each job runs to completion
		
		4. all jobs only use the cpu
		
		5. the run-time of each job is known

	First in, First out
	
	Shortest Job First
	
	Shortest Time-to-Completion First
	
	Round Robin
	
	Incorporating I/O

	if there is no more oracle


SCHEDULING: THE MULTI-LEVEL FEEDBACK QUEUE:

	rules:
		
		1. if priority(a) > priority(b), a runs(b doesn't)
		
		2. if priority(a) = priority(b), a & b run in round-robin fachion using the time slice of the given queue
		
		3. while a job enters the system, it is placed at the highest priority(the topmost queue)
		
		4. once a job uses up its time allotment at a given level(regardless of how many times it has given up the cpu, its prority is reduce
		
		5. after some time period s, move all the jobs in the system to the topmost queue


SCHEDULING: PROPORTIONAL SHARE

	the concept of proportional-share scheduling and briefly diccussed three approaches:
		
		1. lottery scheduling: randomness to achieve proportional share
		
		2. stride scheduling: deterministically
		
		3. the completely fair scheduler of linux: weighted round-robin with dynamic time slices


THE ABSTRACTION: ADDRESS SPACES

	virtual address, physical address

	transparency, efficiency, protection


INTERLUDE: MEMORY API

	malloc(), free() in c language

	forget to allocate memory: segmentation fault
	
	not allocate enough memory: buffer overflow
	
	forget to initialize allocated memory: uninitialized read
	
	forget to free memory: memory leak

	free memory before you are done with it: dangling pointer

	free memory repeatly: double free

	call free() incorrectly: invalid frees


MECHANISM: ADDRESS TRANSLATION

	to attain both efficiency and control while provide the desired virtulization, we use hardware support

	rudimentaryly: just a few registers

	hardware-based address translation(address translation): transforms the virtual address to physical address

	dynamic(hardware-based) relocation: base and bounds

	base and bounds registers are hardware struction kept on the chip

	the part of the processor that helps with the address translation the memory management unit(MMU)

	os need implement base-and-bounds version of virtual memory:

		1. when a process is created, find space for its address space in memory

		2. os must do some work when a process is terminated

		3. os must perform a few additional steps when a context switch occurs 

		4. os must provide exception handlers


SEGMENTATION

	have a base and bounds pair per logical segment of the address space

	a segment is just a contiguous portion of the address space of a particular length

	segmentation allows the os to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space

	calcualte the offset correctly

	an explicit approach to refer to the segments

	support for sharing

	fined-grained and coarse-grained segmentation


FREE-SPACE MANAGEMENT

	now, we focus on external fragmentation

	some common mechanisms used in most allocators:

		1. splitting and coalescing

		2. how to track the size of allocated regions quickly and with relative ease:

			typedef struct __header_t {
				int size;
				int magic; // integrity check
			} header_t;

			void free(void* ptr) {
				header_t* hptr = (void*) ptr - sizeof(header_t);
			}

			the size of the free region is the size of the header plus the size of the space allocated to the user;
			when a user requests N bytes of the memory, the library searches for a free chunk of size N plus the size of the header

		3. how to build a simple list inside the free space to keep track of what is free and what isn't:

			typeded struct __node_t {
				int size;
				struct __node_t *next;
			} node_t;

			// mmap() returns a pointer to a chunk of free space
			node_t* head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0);
			head->size = 4096 - sizeof(node_t);
			head->next = NULL;

	basic strategies:

		1. best fit

		2. worst fit

		3. first fit

		4. next fit

	segregated lists

	buddy allocation


PAGING: INTRODUCTION

	segmentation: chop things up into variable-sized pieces

	paging: chop up space into fixed-sized pieces

	divide virtual memory into prices: page; divide phydical memory into pieces: page frames

	page table: record each virtual page of the address space is placed in physical memory, os keeps a per-process data structure, the major role of page table is to store address translations for each of the virtual pages of the address space

	translate virtual address:
		va => virtual page number(VPN) + offset

		1. turn virtual address into binary form and find the VPN

		2. index the page table and find which physical frame VPN resides within

		3. find physical frame number(PFN, physical page number) and translate va by replacing the VPN with the PFN

	where are these page tables stored?

		page tables are big, don't keep any special on-chip hareware in the MMU to store the page table;
		page tables live in physical memory that os manages 

	what are the typical contents of the page table?

		1. page table is a data structure that is used to map VPN to PFN

		2. simplest form is called a linear page table, which is just an array

		3. the os indexes the array by the VPN and looks up the page-table entry(PTE) in order to find the desired PFN

		4. page-table entry contains PFN, valid bit, protection bits, present bit, dirty bit, reference bit(access bit) and so on

	how big are the tables?

	does paging make the system slow?

		yes!

		movl <va>, %eax (just examine the explicit reference to va and not worry about the instruction fetch):

			1. translate va into the correct physical address

			2. fetch the proper page table entry from the process's page table and perform the translation

			3. fetch the data from pa

			// extract the VPN from the virtual address
			vpn = (virtual_address & vpn_mask) >> shift

			// form the address of the page-table entry
			pteAddr = ptbr + (vpn * sizeof(pte))

			// fetch the pte
			pte = accessMemory(pteAddr)

			// check if process can access the page
			if (pte.valid == false)
				raiseException(SEGMENTATION_FAULT)
			else if (canAccess(pte.protectBits) == false)
				raiseException(PROTECTION_FAULT)
			else
				// access is ok: form physical address and fetch it
				offset = virtualAddress & OFFSET_MASK
				physAddr = (pte.PFN << PFN_SHIFT) | offset
				register = accessMemory(pyhsAddr)


PAGING: FASTER TRANSLATIONS(TLBS)

	to speed address translation, we use translation-lookaside buffer(TLB)

	TLB is part of the chip's memory-management unit(MMU), and is simply a hardware cache of popular virtual-to-physical address translations, so TLB is also called address-translation cache

	TLB basic algorithm:
		VPN = (VirtualAddress & VPN_MASK) >> SHIFT
		(Success, TlbEntry) = TLB_LOOKUP(VPN)
		if (Success == True) // TLB hit
			if (CanAccess(TlbEntry.ProtectBit) == True)
				Offset = VirtualAddress & OFFSET_MASK
				PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
				Register = AccessMemory(PhysAddr)
			else
				RaiseException(PROTECTION_FAULT)
		else
			PTEAddr = PTBR + (VPN * sizeof(PTE))
			PTE = AccessMemory(PTEAddr)
			if (PTE.Valid == False)
				RaiseException(SEGMENTATION_FAULT)
			else if (CanAccess(PTE.ProtectBits) == False)
				RaiseException(PROTECTION_FAULT)
			else
				TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
				RetryInstruction()

	cache is one of the most fundamental performance techniques in computer systems, the idea of hardware cache is to take advantage of locality(temporal locality and spatial locality) in instruction and data references

	if you want a fast cache, it has to be small

	who handles the TLB miss?

		1. hardware(in the olden days): hardware had complex instruction sets(CISC, such as: intel x86 architecture), hardware handle the TLB miss, the hardware has to know exactly where the page tables are located in memory(via a page-table base register), on a miss, the hardware "walk" the page table, find the correct page-table entry and extract the desired instruction

		2. software(os): on a TLB miss, the hardware simply raises an exception which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler

	RISC(reduced instruction set computing), CISC(complex instruction set computing)

	notice: TLB valid bit ISN'T page table valid bit

	TLB has VPN, PFN, and other bits(a valid bit, protection bits, address-space identifier, a dirty bit and so forth)

	when switching between processes(and hence address spaces), some new issues arise:

		1. the TLB contains virtual-to-physical translations that are only valid for the currently running process(meaningless for other processes)

		2. when switching from one process to another, the hardware or os must be careful to ensure the about-to-run process does not accidentally use translations from some previously run process

		3. solutions:

		   	(1): simply flush the TLB on context switches, emptying it before running the next process

		   	(2): some systems add hardware support to enable sharing of the TLB across context switches, such as: provide an address space identifier(ASID), which is just like a process identifier(PID)

	replacement policy(we must consider cache replacement, when we install a new entry, we have to replace an old one, which one to replace?):

		1. least-recently-used(LRU)

		2. random policy


PAGING: SMALLER TABLES

	how to make page table smaller?

		1. bigger pages

		2. paging and segments

		3. multi-level page tables

			VPN = (VirtualAddress & VPN_MASK) >> SHIFT
			(Success, TlbEbtry) = TLB_Lookup(VPN)
			if (Success == True) // TLB hit
				if (CanAccess(TlbEntry.ProtectBits) == True)
					offset = VirtualAddress & OFFSET_MASK
					PhysAddr = (TlbEntry.PFN << SHIFT) | offset
				else
					RaiseException(PROTECTION_FAULT)
			else // TLB Miss
				// first, get page directory entry
				PDIndex = (VPN & PD_MASK) >> PD_SHIFT
				PDEAddr = PDBR + (PDIndex * sizeof(PDE))
				PDE = AccessMemory(PDEAddr)
				if (PDE.Valid == False)
					RaiseException(SEGMENTATION_FAULT)
				else
					// PDE is valid: not fetch PTE from page table
					PTIndex = (VPN & PT_MASK) >> PT_SHIFT
					PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))
					PTE = AccessMemory(PTEAddr)
					if (PTE.Valid == False)
						RaiseException(SEGMENTATION_FAULT)
					else if (CanAccess(PTE.ProtectBits) == False)
						RaiseException(PROTECTION_FAULT)
					else
						TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
						RetryInstruction()

		4. inverted page tables

		5. swapping


BEYOND PHYSICAL MEMORY: MECHANISMS

	how to access more memory than physically present within a system: more complexity in page-table structures:

		1. present bit

		2. page-fault handler to service the page fault

		3. arrange for the transfer of the desired page from disk to memory

		4. these actions all take place transparently to the process


BEYOND PHYSICAL MEMORY: POLICIES

	how to decide whick page to evict: decision is made by the replacement policy of the system

	AMAT(average memory access time): AMAT = Tm + (Pmiss * Td)

	the optimal replacement policy: replace the page that will accessed furthest in the future, which result in the fewest-possible cache miss

	policy:

		1. FIFO(first in, first out): just like a queue

		2. RANDOM

		3. LRU(least-recently-used)

		4. LFU(least-frequently-used)

		5. clock algorithm(approximating LRU): need hardware support: a use bit

		6. consider dirty pages(based on clock algorithm): need hardware support: a dirty bit

		7. other VM policies: prefetching and grouping of writes